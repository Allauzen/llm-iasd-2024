{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCbh4X9_Tiec"
      },
      "source": [
        "# Text classification using Transformers.\n",
        "This lab will focus on text classification on the Imdb dataset.\n",
        "In this lab session, we will use the encoder-based transformer architecture, through the lens of the most famous model: **BERT**.\n",
        "\n",
        "---\n",
        "\n",
        "# Introduction\n",
        "\n",
        "## HuggingFace\n",
        "\n",
        "We have already experimented with some components provided by the HuggingFace library:\n",
        "- the `datasets` library,\n",
        "- the `tokenizer`.\n",
        "\n",
        "Actually, HuggingFace library provides convenient API to deal with transformer models, like BERT, GPT, etc.  To quote their website: *Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. Transformers support framework interoperability between PyTorch, TensorFlow, and JAX.*\n",
        "\n",
        "## Goal of the lab session\n",
        "\n",
        "We will experiment with the HuggingFace library. You'll have to load a model and to run it on your task.\n",
        "\n",
        "Important things to keep in in minds are:\n",
        "- Even if each model is a Transformer, they all have their peculiarities.\n",
        "- What is the exact input format expected by the model?\n",
        "- What is its exact output?\n",
        "- Can you use the available model as is or should you make some modifications for your task?\n",
        "\n",
        "These questions are actually part of the life of a NLP scientist. We will adress some of these questions in this lab and in the next lessons / labs / HW."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee_x9Yq5hTuO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers==4.30"
      ],
      "metadata": {
        "id": "EBXubhnaFEjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuNg51plR2GM"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_formats = ['svg']\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.utils.data import DataLoader\n",
        "from tabulate import tabulate\n",
        "from datasets import load_dataset\n",
        "# pretrained\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# If the machine you run this on has a GPU available with CUDA installed,\n",
        "# use it. Using a GPU for learning often leads to huge speedups in training.\n",
        "# See https://developer.nvidia.com/cuda-downloads for installing CUDA\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkOsT1Bttqod"
      },
      "source": [
        "## Download the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA6soGbGR__h"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"scikit-learn/imdb\", split=\"train\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAVqumxwXc1d"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S88giEYTiel"
      },
      "source": [
        "## Prepare model inputs\n",
        "\n",
        "The input format to BERT looks like it is  \"over-specified\", especially if you focus on just one type task: sequence classification, word tagging, paraphrase detection, etc. The format:\n",
        "- Add special tokens to the start and end of each sentence.\n",
        "- Pad & truncate all sentences to a single constant length.\n",
        "- Explicitly differentiate real tokens from padding tokens with the \"attention mask\".\n",
        "\n",
        "It looks like that:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1cb5xeqLu_5vPOgs3eRnail2Y00Fl2pCo\" width=\"600\">\n",
        "\n",
        "If you don't want to recreate this kind of inputs with your own hands, you can use the pre-trained tokenizer associated to BERT. Moreover the tokenizer will:\n",
        "- Tokenize the sentence.\n",
        "- Prepend the `[CLS]` token to the start.\n",
        "- Append the `[SEP]` token to the end.\n",
        "- Map tokens to their IDs.\n",
        "- Pad or truncate the sentence to `max_length`\n",
        "- Create attention masks for `[PAD]` tokens.\n",
        "\n",
        "\n",
        "> ðŸ’¡ *Note:* For computational reasons, we will use the [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) model, which is a 40% smaller than the original BERT model but still achieve about 95% of the performances of the original model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sUff5oag4SP"
      },
      "outputs": [],
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained(\n",
        "    \"distilbert-base-uncased\", do_lower_case=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9PJQH0yZbE0"
      },
      "source": [
        "Let's see how the tokenizer actually process the sequence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXpTeR-mTiem"
      },
      "outputs": [],
      "source": [
        "# Some useful steps:\n",
        "message = \"hello my name is footballman\"\n",
        "tok = tokenizer.tokenize(message)\n",
        "print(\"Tokens in the sequence:\", tok)\n",
        "enc = tokenizer.encode(tok)\n",
        "table = np.array(\n",
        "    [\n",
        "        enc,\n",
        "        [tokenizer.ids_to_tokens[w] for w in enc],\n",
        "    ]\n",
        ").T\n",
        "print(\"Encoded inputs:\")\n",
        "print(tabulate(table, headers=[\"Token IDs\", \"Tokens\"], tablefmt=\"fancy_grid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv_6QpsPTien"
      },
      "source": [
        "ðŸš§ **Question** ðŸš§\n",
        "\n",
        "You noticed special tokens like `[CLS]` and `[SEP]` in the sequence. Note how they were added automatically by HuggingFace.\n",
        "\n",
        "- Why are there such special tokens?\n",
        "\n",
        "**Answer**\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SncaMoJP9JmA"
      },
      "source": [
        "ðŸš§ **TODO** ðŸš§\n",
        "\n",
        "- Run the code below to make sure you can control this behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gQU4w-1bnh-"
      },
      "outputs": [],
      "source": [
        "text = \"my name is kevin\"\n",
        "tokenized_text_without_special_tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "print(tokenized_text_without_special_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJJDGDDZc49_"
      },
      "source": [
        "## Data pre-processing\n",
        "\n",
        "Usual data-processing for torch. Same as previous lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv4YSE9Zg4SP"
      },
      "outputs": [],
      "source": [
        "def preprocessing_fn(x, tokenizer):\n",
        "    x[\"input_ids\"] = tokenizer.encode(\n",
        "        x[\"review\"],\n",
        "        add_special_tokens=False,\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        padding=False,\n",
        "        return_attention_mask=False,\n",
        "    )\n",
        "    x[\"labels\"] = 0 if x[\"sentiment\"] == \"negative\" else 1\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx2KZ7AFXPYU"
      },
      "outputs": [],
      "source": [
        "n_samples = 2000  # the number of training example\n",
        "\n",
        "# We first shuffle the data !\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "# Select n_samples\n",
        "splitted_dataset = dataset.select(range(n_samples))\n",
        "\n",
        "# Tokenize the dataset\n",
        "splitted_dataset = splitted_dataset.map(\n",
        "    preprocessing_fn, fn_kwargs={\"tokenizer\": tokenizer}\n",
        ")\n",
        "\n",
        "\n",
        "# Remove useless columns\n",
        "splitted_dataset = splitted_dataset.select_columns([\"input_ids\", \"labels\"])\n",
        "\n",
        "# Split the train and validation\n",
        "splitted_dataset = splitted_dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "train_set = splitted_dataset[\"train\"]\n",
        "valid_set = splitted_dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set[0][\"labels\"]"
      ],
      "metadata": {
        "id": "YEGnSDaqCSOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwVeJ9-jdJbF"
      },
      "outputs": [],
      "source": [
        "class DataCollator:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # `batch` is a list of dictionary with keys \"review_ids\" and \"label\".\n",
        "        features = self.tokenizer.pad(\n",
        "            batch, padding=\"longest\", max_length=256, return_tensors=\"pt\"\n",
        "        )\n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gGBxVl9Cukq"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollator(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVuKFSa9g4SY"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_set, batch_size=batch_size, collate_fn=data_collator\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    valid_set, batch_size=batch_size, collate_fn=data_collator\n",
        ")\n",
        "n_valid = len(valid_set)\n",
        "n_train = len(train_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmdqqdiAeHJz"
      },
      "source": [
        "# Model from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz7ith48eKQo"
      },
      "source": [
        "For this first exercise, we will start from a randomly initialized model.\n",
        "\n",
        "## Retrieve the architecture configuration\n",
        "\n",
        "In HuggingFace, model's parameters are specified through a `config` file. It is a json-like object.\n",
        "\n",
        "We can retrieve the one from the official model with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV_TxzGkesZO"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertConfig\n",
        "\n",
        "model_config = DistilBertConfig.from_pretrained(\"distilbert-base-uncased\")\n",
        "print(model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7kjUw-CfXSD"
      },
      "source": [
        "ðŸš§ **Question** ðŸš§\n",
        "\n",
        "Make sure you understand the parameters of the configuration.\n",
        "- Which ones are task-agnostic parameters?\n",
        "- Which ones are not?\n",
        "- Why are there different parameters for different tasks?\n",
        "\n",
        "**Answer**\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl6WP4URiCtv"
      },
      "source": [
        "\n",
        "\n",
        "Several architectures are available for DistilBert on HuggingFace, designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained DistilBert model, each has different top layers and output types designed to accomodate their specific NLP task.  \n",
        "\n",
        "Here is the current list of classes provided for fine-tuning:\n",
        "* BertModel\n",
        "* BertForMaskedLM\n",
        "* BertForNextSentencePrediction\n",
        "* BertForSequenceClassification\n",
        "* BertForTokenClassification\n",
        "* BertForQuestionAnswering\n",
        "\n",
        "The documentation for these can be found under [here](https://huggingface.co/docs/transformers/model_doc/distilbert).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5ae34icijwy"
      },
      "source": [
        "ðŸš§ **TODO** ðŸš§\n",
        "\n",
        "For our first experiment, we want to build from a standard stack of transformer layers, without any additional task-specific head.\n",
        "\n",
        "Which architecture is the corresponding one?\n",
        "\n",
        "Choose the right one and initialize the model below, with the config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVyMo6wZhR_e"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertModel\n",
        "\n",
        "bert = DistilBertModel(model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9G9xJ1HOjDiO",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "print(bert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0lR6ZzJTieq"
      },
      "source": [
        "Just for curiosity's sake, we can browse all of the model's parameters by name here.\n",
        "\n",
        "In the cell below, we printed out the names and dimensions of the weights for:\n",
        "\n",
        "- The embedding layer\n",
        "- The first of the twelve transformers\n",
        "- The output layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p971QtCrTieq"
      },
      "outputs": [],
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(bert.named_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgUGt9LNTier"
      },
      "outputs": [],
      "source": [
        "print(\"The BERT model has {:} different named parameters.\\n\".format(len(params)))\n",
        "\n",
        "print(\"==== Embedding Layer ====\\n\")\n",
        "\n",
        "for p in params[0:4]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print(\"\\n==== First Transformer Layer ====\\n\")\n",
        "\n",
        "for p in params[4:20]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCYvYRo9Tier"
      },
      "source": [
        "ðŸš§ **TODO** ðŸš§\n",
        "\n",
        "Test your `bert`.\n",
        "We can already try the model on the validation set. Before just look at the output of the model on one batch.\n",
        "- Interpret the output.\n",
        "- Do you understand everything ?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r_glwGiTier"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "\n",
        "input_ids = batch[\"input_ids\"]\n",
        "attention_mask = batch[\"attention_mask\"]\n",
        "output = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "print(output[\"last_hidden_state\"].shape)\n",
        "print(type(output))\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq8-YW5in6EC"
      },
      "source": [
        "## Building a classifier\n",
        "\n",
        "Our `bert` model is simply a stack of transformer layers. We would like to use it as a backbone for text classification.\n",
        "\n",
        "ðŸš§ **TODO** ðŸš§\n",
        "\n",
        "Wraps the model into a classifier.\n",
        "\n",
        "> ðŸ’¡ *Hint*: Use the last hidden [CLS] vector representation to perform classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EULTqBYSnc7P"
      },
      "outputs": [],
      "source": [
        "class DistilBertClassifier(nn.Module):\n",
        "    def __init__(self, bert, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.dp = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        out = bert_out[\"last_hidden_state\"]\n",
        "        cls_embed = out[:, 0]\n",
        "        out = self.dp(cls_embed)\n",
        "        return self.classifier(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5Q4wA7SpPIO"
      },
      "outputs": [],
      "source": [
        "bert = DistilBertModel(model_config)\n",
        "model = DistilBertClassifier(bert)\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3owzcrFy9JmD"
      },
      "source": [
        "ðŸš§ **TODO** ðŸš§\n",
        "\n",
        "Test your model on the batch.\n",
        "Make sure it has the right shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBpkEqeQpgVV"
      },
      "outputs": [],
      "source": [
        "out = model.forward(input_ids.to(DEVICE), attention_mask.to(DEVICE))\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeirXUhL9JmE"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nut59Bw09JmE"
      },
      "source": [
        "ðŸš§ **TODO** ðŸš§\n",
        "\n",
        "Train your model.\n",
        "Make sure you track the following quantities per epoch:\n",
        "- training loss\n",
        "- training accuracy\n",
        "- validation loss\n",
        "- validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUVE8hfzJD0u"
      },
      "outputs": [],
      "source": [
        "# Redefine the dataloaders to adjust the batch size.\n",
        "batch_size = 16\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_set, batch_size=batch_size, collate_fn=data_collator\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    valid_set, batch_size=batch_size, collate_fn=data_collator\n",
        ")\n",
        "n_valid = len(valid_set)\n",
        "n_train = len(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7ZZKxFv9JmE"
      },
      "outputs": [],
      "source": [
        "def validation(model, valid_dataloader):\n",
        "    total_size = 0\n",
        "    acc_total = 0\n",
        "    loss_total = 0\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_dataloader):\n",
        "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "            input_ids = batch[\"input_ids\"]\n",
        "            labels = batch[\"labels\"]\n",
        "            attention_mask = batch[\"attention_mask\"]\n",
        "            labels = labels.float()\n",
        "            preds = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(preds.squeeze(), labels)\n",
        "            acc = (preds.squeeze() > 0) == labels\n",
        "            total_size += acc.shape[0]\n",
        "            acc_total += acc.sum().item()\n",
        "            loss_total += loss.item()\n",
        "    model.train()\n",
        "    return loss_total / len(valid_dataloader), acc_total / total_size\n",
        "\n",
        "\n",
        "validation(model, valid_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OH-tclui9JmE"
      },
      "outputs": [],
      "source": [
        "def training(model, n_epochs, train_dataloader, valid_dataloader, lr=5e-5):\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        eps=1e-08,\n",
        "    )\n",
        "    list_val_acc = []\n",
        "    list_train_acc = []\n",
        "    list_train_loss = []\n",
        "    list_val_loss = []\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    for e in range(n_epochs):\n",
        "        # ========== Training ==========\n",
        "\n",
        "        # Set model to training mode\n",
        "        model.train()\n",
        "        model.to(DEVICE)\n",
        "\n",
        "        # Tracking variables\n",
        "        train_loss = 0\n",
        "        epoch_train_acc = 0\n",
        "        for batch in tqdm(train_dataloader):\n",
        "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "            input_ids, attention_mask, labels = (\n",
        "                batch[\"input_ids\"],\n",
        "                batch[\"attention_mask\"],\n",
        "                batch[\"labels\"],\n",
        "            )\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            # Backward pass\n",
        "            loss = criterion(outputs.squeeze(), labels.squeeze().float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.detach().cpu().item()\n",
        "            acc = (outputs.squeeze() > 0) == labels.squeeze()\n",
        "            epoch_train_acc += acc.float().mean().item()\n",
        "        list_train_acc.append(100 * epoch_train_acc / len(train_dataloader))\n",
        "        list_train_loss.append(train_loss / len(train_dataloader))\n",
        "\n",
        "        # ========== Validation ==========\n",
        "\n",
        "        l, a = validation(model, valid_dataloader)\n",
        "        list_val_loss.append(l)\n",
        "        list_val_acc.append(a * 100)\n",
        "        print(\n",
        "            e,\n",
        "            \"\\n\\t - Train loss: {:.4f}\".format(list_train_loss[-1]),\n",
        "            \"Train acc: {:.4f}\".format(list_train_acc[-1]),\n",
        "            \"Val loss: {:.4f}\".format(l),\n",
        "            \"Val acc:{:.4f}\".format(a * 100),\n",
        "        )\n",
        "    return list_train_loss, list_train_acc, list_val_loss, list_val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DNYevBd9JmF"
      },
      "outputs": [],
      "source": [
        "training(model, 3, train_dataloader, valid_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6lfOwHaTiex"
      },
      "source": [
        "## Pre-trained model\n",
        "\n",
        "Now we are going to compare with a pre-trained model.\n",
        "\n",
        "First, we are going to load the model's weights from the HuggingFace hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cZ3cG8zOk-o"
      },
      "outputs": [],
      "source": [
        "bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEOXEapzPLRB"
      },
      "source": [
        "## Fine-Tuning\n",
        "\n",
        "With our model loaded and ready,  we need to grab the training hyperparameters from within the stored model.\n",
        "\n",
        "For the purposes of fine-tuning, the authors recommend choosing from the following values (from Appendix A.3 of the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf)):\n",
        "\n",
        "- Batch size: 16, 32  \n",
        "- Learning rate (Adam): 5e-5, 3e-5, 2e-5  \n",
        "- Number of epochs: 2, 3, 4\n",
        "\n",
        "We chose:\n",
        "* **Batch size**: 16 (set when creating our DataLoaders)\n",
        "* **Learning rate**: 5e-5\n",
        "* **Epochs**: 3 (we'll see that this is probably too many...)\n",
        "\n",
        "The epsilon parameter `eps = 1e-8` is \"a very small number to prevent any division by zero in the implementation\" (from [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
        "\n",
        "You can find the creation of the AdamW optimizer in `run_glue.py` [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1nJVGZcPbvc"
      },
      "source": [
        "ðŸš§ **TODO** ðŸš§\n",
        "\n",
        "Build the classifier and train it with the pre-trained checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs4m6GDhOrYa"
      },
      "outputs": [],
      "source": [
        "model = DistilBertClassifier(bert.to(DEVICE))\n",
        "training(model, 3, train_dataloader, valid_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P3l-VonPuZ5"
      },
      "source": [
        "ðŸš§ **Question** ðŸš§\n",
        "\n",
        "What do you think of the results?\n",
        "\n",
        "**Answer**\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7hIOyN9P1XH"
      },
      "source": [
        "## Pre-built models\n",
        "\n",
        "Actually, you built your own classifier based on the raw output of a transformers, wrapped into a classification model.\n",
        "\n",
        "But, for many tasks, you can directly download the model with the necessary blocks.\n",
        "\n",
        "For instance, we could directly have loaded `DistilBertForSequenceClassification`.\n",
        "\n",
        "Let's see the difference with our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gcajoi5QP2h"
      },
      "source": [
        "ðŸš§ **TODO** ðŸš§\n",
        "\n",
        "Load a pre-trained `DistilBertForSequenceClassification`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTo8G7O4QVSS"
      },
      "outputs": [],
      "source": [
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=1,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False,\n",
        ")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8zftLvcTieq"
      },
      "source": [
        "ðŸš§ **Question** ðŸš§\n",
        "\n",
        "Here there might be a lot of questions:\n",
        "- what does the warning means?\n",
        "- why `num_labels=1`?\n",
        "- and the other options?\n",
        "\n",
        "**Answer**\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aVaPsm29JmH"
      },
      "source": [
        "ðŸš§ **Question** ðŸš§\n",
        "\n",
        "How is the classifcation made? Is it the same than with our model? You might need to check the official implementation [here](https://github.com/huggingface/transformers/blob/v4.34.1/src/transformers/models/distilbert/modeling_distilbert.py#L730).\n",
        "\n",
        "**Answer**\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fln9lWhETVV9"
      },
      "source": [
        "ðŸš§ **TODO** ðŸš§\n",
        "\n",
        "The output of such a model is not directly the logits, but a wrapper that can return several objects. Analyze it and modify the training and validation loop accordingly.\n",
        "\n",
        "Launch the training.\n",
        "\n",
        "Do you observe any differences with our own classification model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5MBNbd0SPIa"
      },
      "outputs": [],
      "source": [
        "model.to(DEVICE)\n",
        "out = model(\n",
        "    input_ids=batch[\"input_ids\"].to(DEVICE),\n",
        "    attention_mask=batch[\"attention_mask\"].to(DEVICE),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validation_hf(model, valid_dataloader):\n",
        "    total_size = 0\n",
        "    acc_total = 0\n",
        "    loss_total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_dataloader):\n",
        "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "            input_ids = batch[\"input_ids\"]\n",
        "            labels = batch[\"labels\"]\n",
        "            attention_mask = batch[\"attention_mask\"]\n",
        "            labels = labels.float()\n",
        "            out = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=batch[\"labels\"],\n",
        "            )\n",
        "            loss = out.loss\n",
        "            acc = (out.logits.squeeze() > 0) == labels\n",
        "            total_size += acc.shape[0]\n",
        "            acc_total += acc.sum().item()\n",
        "            loss_total += loss.item()\n",
        "    model.train()\n",
        "    return loss_total / len(valid_dataloader), acc_total / total_size\n",
        "\n",
        "def training_hf(model, n_epochs, train_dataloader, valid_dataloader, lr=5e-5):\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        eps=1e-08,\n",
        "    )\n",
        "    list_val_acc = []\n",
        "    list_train_acc = []\n",
        "    list_train_loss = []\n",
        "    list_val_loss = []\n",
        "    for e in range(n_epochs):\n",
        "        # ========== Training ==========\n",
        "\n",
        "        # Set model to training mode\n",
        "        model.train()\n",
        "        model.to(DEVICE)\n",
        "\n",
        "        # Tracking variables\n",
        "        train_loss = 0\n",
        "        epoch_train_acc = 0\n",
        "        for batch in tqdm(train_dataloader):\n",
        "            batch = {k: v.cuda() for k, v in batch.items()}\n",
        "            input_ids, attention_mask, labels = (\n",
        "                batch[\"input_ids\"],\n",
        "                batch[\"attention_mask\"],\n",
        "                batch[\"labels\"],\n",
        "            )\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            out = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels.float(),\n",
        "            )\n",
        "            # Backward pass\n",
        "            loss = out.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.detach().cpu().item()\n",
        "            acc = (out.logits.squeeze() > 0) == labels.squeeze()\n",
        "            epoch_train_acc += acc.float().mean().item()\n",
        "        list_train_acc.append(100 * epoch_train_acc / len(train_dataloader))\n",
        "        list_train_loss.append(train_loss / len(train_dataloader))\n",
        "\n",
        "        # ========== Validation ==========\n",
        "\n",
        "        l, a = validation_hf(model, valid_dataloader)\n",
        "        list_val_loss.append(l)\n",
        "        list_val_acc.append(a * 100)\n",
        "        print(\n",
        "            e,\n",
        "            \"\\n\\t - Train loss: {:.4f}\".format(list_train_loss[-1]),\n",
        "            \"Train acc: {:.4f}\".format(list_train_acc[-1]),\n",
        "            \"Val loss: {:.4f}\".format(l),\n",
        "            \"Val acc:{:.4f}\".format(a * 100),\n",
        "        )\n",
        "    return list_train_loss, list_train_acc, list_val_loss, list_val_acc"
      ],
      "metadata": {
        "id": "n0wxXLuCZNzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_train_loss, list_train_acc, list_val_loss, list_val_acc = training_hf(\n",
        "    model, 3, train_dataloader, valid_dataloader, lr=3e-5\n",
        ")"
      ],
      "metadata": {
        "id": "ubHlKTNLZUhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgNpJOLQWOIc"
      },
      "source": [
        "# Interpretability\n",
        "\n",
        "So far we have models able to predict quite faithfully if a critic is positive or negative. But can we interprete the results?\n",
        "\n",
        "A usual way to do so with transformers, is simply to look at the **attention weights**. Let's see if we can get some insights on the model's prediction using this technique.\n",
        "\n",
        "\n",
        "First, tokenize the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnI-s0FiWPj9"
      },
      "outputs": [],
      "source": [
        "text = (\n",
        "    \"captain corelli's mandolin is a beautiful film with a lovely cast\"\n",
        "    \" including the wonderful nicolas cage, who as always is brilliant in the movie.\"\n",
        "    \" the music in the film is really nice too. i'd advise anyone to go and see it. brilliant! 10 / 10 \"\n",
        ")\n",
        "tokenized_text = tokenizer(text, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJpghTA2eagk"
      },
      "source": [
        "ðŸš§ **TODO** ðŸš§\n",
        "\n",
        "Now feed it as an output for the model. Use the keyword-argument `output_attentions=True`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_VgUge-ZX_S"
      },
      "outputs": [],
      "source": [
        "model_output = model(\n",
        "    input_ids=tokenized_text[\"input_ids\"].to(DEVICE), output_attentions=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHPG4bdoeqRp"
      },
      "source": [
        "ðŸš§ **TODO** ðŸš§\n",
        "Check the prediction and plot the attention weights matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHxGfslz9JmI"
      },
      "outputs": [],
      "source": [
        "print(\"Logits:\", model_output.logits.item())\n",
        "print(\"Prediction:\", int((model_output.logits > 0).item()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_attention(attention, tokenized_text, tokenizer, layer=0):\n",
        "    if layer == \"all\":\n",
        "        attention_array = np.concatenate(\n",
        "            [layer.detach().cpu().numpy() for layer in attention]\n",
        "        )\n",
        "    if isinstance(layer, int):\n",
        "        attention_array = attention[layer].detach().cpu().numpy()\n",
        "    if isinstance(layer, list):\n",
        "        attention_array = np.concatenate(\n",
        "            [attention[i].detach().cpu().numpy() for i in layer]\n",
        "        )\n",
        "    attention_array = attention_array.max(axis=(0, 1))\n",
        "    fig, axs = plt.subplots(1, 1, figsize=(10, 10))\n",
        "    axs.imshow(attention_array)\n",
        "    tokens = [\n",
        "        tokenizer.ids_to_tokens[w] for w in tokenized_text[\"input_ids\"][0].tolist()\n",
        "    ]\n",
        "    axs.set_xticks(np.arange(len(tokens)), tokens, rotation=\"vertical\", fontsize=8)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "print_attention(model_output[\"attentions\"], tokenized_text, tokenizer, layer=4)"
      ],
      "metadata": {
        "id": "sR5r9hQeZdwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY9OK9InfXwk"
      },
      "source": [
        "ðŸš§ **Question** ðŸš§\n",
        "\n",
        "Is your interpretability experiment conclusive?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm-course-7UlZO0np-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}