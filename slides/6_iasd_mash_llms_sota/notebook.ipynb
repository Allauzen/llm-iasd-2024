{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRdXCXGf5xeI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT MLM"
      ],
      "metadata": {
        "id": "kTPB8eoFuL63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer  = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "z3_N2I9P59io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for _ in range(10):\n",
        "    input_ids = torch.randint(0, len(tokenizer), (16, 256))\n",
        "    data.append(input_ids)\n"
      ],
      "metadata": {
        "id": "vGIungJZ6VPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert.cuda();"
      ],
      "metadata": {
        "id": "XJY0tuwn5ctt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "942uyjNU6Nyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(bert.parameters(), lr=1e-3)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for input_ids in data:\n",
        "    out = bert(input_ids=input_ids.cuda(), labels=input_ids.cuda())\n",
        "    out.loss.backward()\n",
        "    optimizer.step()\n",
        "end = time.time()\n",
        "print((end-start) / 10.0)"
      ],
      "metadata": {
        "id": "ijq0NrPw6AMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Time for 1M steps (in days):\", int(1e6 * (end-start) / 10 / 3600 / 24))"
      ],
      "metadata": {
        "id": "MUReJElg8L_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient accumulation"
      ],
      "metadata": {
        "id": "bMWhF8ZBudz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(theta):\n",
        "    return (0.5 * theta**2).sum()"
      ],
      "metadata": {
        "id": "LbjQcRVwudYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have:\n",
        "\\begin{equation}\n",
        "\\dfrac{\\partial}{\\partial \\theta_i} f(\\theta) = \\dfrac{\\partial}{\\partial \\theta_i}  \\sum_{i=1}^n\\dfrac{1}{2} \\theta_i^2 = \\theta_i.\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "0cEDB_9EvFN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta = torch.arange(10, dtype=float, requires_grad=True)\n",
        "theta"
      ],
      "metadata": {
        "id": "684042sx6CUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta = torch.arange(10, dtype=float, requires_grad=True)\n",
        "\n",
        "loss = f(theta)\n",
        "loss.backward()\n",
        "print(\"Gradient attached to theta:\", theta.grad)"
      ],
      "metadata": {
        "id": "5mvRrQWDun3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta = torch.arange(10, dtype=float, requires_grad=True)\n",
        "print(\"Initial gradient:\", theta.grad)\n",
        "for i in range(2):\n",
        "    loss = f(theta)\n",
        "    loss.backward()\n",
        "    input(\"Continue\")\n",
        "    print(f\"Gradient attached to theta at step {i+1}:\", theta.grad)"
      ],
      "metadata": {
        "id": "HIUCR1jZvZYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A gradient accumulation is readily performed like:"
      ],
      "metadata": {
        "id": "N3ZjSOlSwDFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bert = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer  = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "16_u0A42wCi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "batch_size = 2\n",
        "for _ in range(10):\n",
        "    input_ids = torch.randint(0, len(tokenizer), (batch_size, 8))\n",
        "    data.append(input_ids)"
      ],
      "metadata": {
        "id": "_6cKEWhywLsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert.cuda();"
      ],
      "metadata": {
        "id": "26XD85aYxBbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(bert.parameters(), lr=1e-3)\n",
        "optimizer.zero_grad()\n",
        "\n",
        "iteration_steps = 0\n",
        "optimization_steps = 0\n",
        "gradient_accumulation = 2\n",
        "\n",
        "for input_ids in data:\n",
        "    out = bert(input_ids=input_ids.cuda(), labels=input_ids.cuda())\n",
        "    loss = out.loss\n",
        "\n",
        "    loss = loss / gradient_accumulation # To average the gradient, otherwise it performs summation.\n",
        "    loss.backward()\n",
        "\n",
        "    iteration_steps += 1\n",
        "\n",
        "    if (iteration_steps % gradient_accumulation) == 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        optimization_steps += 1\n",
        "\n",
        "print(\"Total number of data iterations:\", iteration_steps)\n",
        "print(\"Total number of opimization steps:\", optimization_steps)"
      ],
      "metadata": {
        "id": "7PTgwjSJwOJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2-Large memory requirements\n",
        "\n",
        "Make sure to free the cuda memory before running this (you can relaunch the notebook for instance)."
      ],
      "metadata": {
        "id": "aiZf8Gzr1kSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "xPCuSJwD3Dgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-large\")\n"
      ],
      "metadata": {
        "id": "pTfaZTpL1jqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt = gpt.cuda();\n",
        "num_params = sum(p.numel() for p in gpt.parameters())\n",
        "print(f\"The number of parameters of GPT2-Large is: {num_params}\")"
      ],
      "metadata": {
        "id": "RQXtOyL12CIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "zGQFQkFv2E5K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}