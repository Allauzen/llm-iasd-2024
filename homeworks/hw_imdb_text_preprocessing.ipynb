{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-8ltYHfbs5B"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laDowDK-m6Ag"
   },
   "source": [
    "# HW Instructions\n",
    "\n",
    "## 🚧 Cells\n",
    "All the cells identified with a 🚧 should be completed, either by a code or an written answer.\n",
    "You can add as many cells as you want, but in general cells are already in place.\n",
    "\n",
    "\n",
    "🚧 **TODO** 🚧\n",
    "\n",
    "*Requires to complete a code (can be completed with optional text cells if you find it relevant).*\n",
    "\n",
    "or\n",
    "\n",
    "🚧 **Question** 🚧\n",
    "\n",
    "*Requires a written answer (can be completed with optional code cells if you find it relevant).*\n",
    "\n",
    "## 🔴 Test cells\n",
    "Cells identified with 🔴 should be left untouched and shouldn't return any error.\n",
    "\n",
    "## Presentation\n",
    "The overall presentation of the notebook will account in the grading process.\n",
    "Some advice:\n",
    "- Give title to your figures.\n",
    "- Put legends on the figures.\n",
    "- Comment the figures if they are not self-explanatory.\n",
    "- Add comment in the code if not self-explanatory.\n",
    "- Review your notebook before submitting.\n",
    "- Feel free to add any additional illustration if you find it will bring something.\n",
    "- Remain as concise as possible when answering written answer, emphasize the important points.\n",
    "\n",
    "**We cannot debug every notebooks. The notebook should run entirely from the start to the end without any error. If one cell returns an error during the execution, only the cells before the error will be graded.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaBJpPWfW45L"
   },
   "source": [
    "# HW Introduction\n",
    "\n",
    "## Hands on with NLP preprocessing\n",
    "\n",
    "The goal of this first homework is to introduce the preprocessing steps in NLP. This step is necessary, very valuable but often not emphasized enough (because it is boring). \n",
    "\n",
    "## Plan\n",
    "\n",
    "1. Load the data\n",
    "2. Analyze and clean the text data\n",
    "3. Format the texts for Deep Learning models and pytorch (i.e. tokenize, etc).\n",
    "\n",
    "## Notations\n",
    "\n",
    "Throughout the HW, a text will be called either **document** or **text**.\n",
    "\n",
    "\n",
    "# I - Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eri6thqeX_MC"
   },
   "outputs": [],
   "source": [
    "def download_imdb(output_path: str, force_download: bool = False):\n",
    "    \"\"\"Save the data to `output_path`.\"\"\"\n",
    "    output_path = Path(output_path)\n",
    "    if (not output_path.is_file()) or force_download:\n",
    "        gdd.download_file_from_google_drive(\n",
    "            file_id=\"1zfM5E6HvKIe7f3rEt1V2gBpw5QOSSKQz\",\n",
    "            dest_path=output_path,\n",
    "        )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJ9S1a1jW45M"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/imdb_reviews.csv\"\n",
    "download_imdb(output_path=DATA_PATH, force_download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGT4vDXmZDTT"
   },
   "source": [
    "🚧 **TODO** 🚧\n",
    "\n",
    "Read the file to a Pandas dataframe (the file is stored as a csv). Make sure the \"review\" field is of dtype `string` and label of dtype `int`.\n",
    "\n",
    "Hint: https://pandas.pydata.org/docs/user_guide/text.html (you can specify the `dtype` argument of pandas constructor).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbQw5TGPha20"
   },
   "outputs": [],
   "source": [
    "df_imdb = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAv44K5tpMHF"
   },
   "outputs": [],
   "source": [
    "text_column = df_imdb[\"review\"]\n",
    "label_column = df_imdb[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqaE3bBDpPVK"
   },
   "source": [
    "🔴 **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gMHYuJHcQB5"
   },
   "outputs": [],
   "source": [
    "assert text_column.dtype == \"string\"\n",
    "assert label_column.dtype == int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMTjOSWEbM6Q"
   },
   "source": [
    "🚧 **TODO** 🚧\n",
    "\n",
    "Print some values of the dataframe.\n",
    "- Print 3 samples of text.\n",
    "- Print their associated labels.\n",
    "- Print some statistics of the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-aQ0d6IIgRO_"
   },
   "outputs": [],
   "source": [
    "print(\"==== 3 first texts ====\")\n",
    "# TODO\n",
    "\n",
    "\n",
    "print(\"\\n==== Associated labels of the 3 first texts ====\")\n",
    "# TODO\n",
    "\n",
    "\n",
    "print(\"\\n=== Dataset statistics ===\")\n",
    "print(\"Number of texts\")\n",
    "# TODO\n",
    "\n",
    "\n",
    "print(\"\\nAverage text length (in characters)\")\n",
    "# TODO\n",
    "\n",
    "\n",
    "print(\"\\n% of positive texts\")\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ON1bQK5jhz4E"
   },
   "source": [
    "## II - Clean and format the data.\n",
    "For this first TP, we want to focus on very simple tasks.\n",
    "Therefore, we want to avoid as much as possible rare and uncommon words that will be considered as outliers.\n",
    "\n",
    "We will try to figure out what kind of cleaning we can perform, by having a look at the data.\n",
    "\n",
    "The plan for this part will be:\n",
    "1. Split the strings as a list of words.\n",
    "2. Find the outliers in the corpus.\n",
    "3. Propose some methods to clean them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XLRgi5giy_g"
   },
   "source": [
    "### 1. Tokenization\n",
    "First, we will split our texts into words.\n",
    "Splitting a string into a list of smaller substrings is called **tokenization**, and the substrings are called **tokens**.\n",
    "\n",
    "The simplest way to tokenize a text is to split it by words, according to whitespaces.\n",
    "\n",
    "Then, each word encountered in the training set is stored and uniquely identified through an id.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-HLOTivIfSP"
   },
   "source": [
    "🚧 **Question** 🚧\n",
    "\n",
    "By doing so, we put a huge emphasis on words within documents.\n",
    "\n",
    "a) Intuitively, why is it relevant? For instance compare to splitting by character.\n",
    "\n",
    "b) Cite some methods discussed during the course that rely on word-based representations of documents.\n",
    "\n",
    "**Answer**\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYwJDXAUIc7C"
   },
   "source": [
    "🚧 **TODO** 🚧\n",
    "\n",
    "Implement a tokenizer based on whitespace splitting. For now, the tokenizer should only be able to store each unique word found in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "791KTHn9W45N"
   },
   "outputs": [],
   "source": [
    "class WhiteSpaceTokenizer:\n",
    "    def __init__(self):\n",
    "        # The vocabulary will store the mapping between text tokens and their id.\n",
    "        self.vocab = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "        # We will keep track of the number of times a word appears in the corpus.\n",
    "        self.frequencies = {}\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Converts the text to a list of tokens (substrings).\"\"\"\n",
    "        # TODO\n",
    "\n",
    "    def fit(self, corpus: List[str]):\n",
    "        \"\"\"Fits the tokenizer to a list of texts to construct its vocabulary.\"\"\"\n",
    "        current_id = 0\n",
    "        for text in tqdm(corpus):\n",
    "\n",
    "            # Split into substrings.\n",
    "            list_tokens = #TODO\n",
    "\n",
    "            for token in list_tokens:\n",
    "\n",
    "                # Add the token to the vocabulary\n",
    "                token_id = self.vocab.get(token, None)\n",
    "\n",
    "                token_not_in_vocab = #TODO\n",
    "                if token_not_in_vocab:\n",
    "                    self.vocab[token] = #TODO\n",
    "                    self.id_to_token[current_id] = #TODO\n",
    "                    self.frequencies[current_id] = 0\n",
    "                    token_id = current_id\n",
    "                    current_id += 1\n",
    "\n",
    "                self.frequencies[token_id] += 1\n",
    "\n",
    "        # Total number of words in vocab.\n",
    "        self.num_words = #TODO\n",
    "        print(f\"Built a vocabulary of {self.num_words} words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geTlVAEGpa4n"
   },
   "source": [
    "🔴 **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joZuRcZ7nc3N"
   },
   "outputs": [],
   "source": [
    "toy_corpus = [\"a cat\", \"a dog\"]\n",
    "tokenizer = WhiteSpaceTokenizer()\n",
    "tokenizer.fit(toy_corpus)\n",
    "assert tokenizer.vocab == {\"a\": 0, \"cat\": 1, \"dog\": 2}\n",
    "assert tokenizer.frequencies == {0: 2, 1: 1, 2: 1}\n",
    "assert tokenizer.num_words == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fct0L_9EoGWT"
   },
   "source": [
    "🚧 **TODO** 🚧\n",
    "\n",
    "Fits the tokenizer on the imdb reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59qiHgwmmcfB"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHBeN-x-2fhB"
   },
   "source": [
    "### 2. Noise analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3H-GBNNVobV8"
   },
   "source": [
    "🚧 **TODO** 🚧\n",
    "\n",
    "Print 50 random tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AI2knE7em9HT"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buQNo3ZsrXq4"
   },
   "source": [
    "🚧 **QUESTION** 🚧\n",
    "\n",
    "What do you think of the tokens? Do they look like simple \"words\" as expected?\n",
    "\n",
    "Analyze the noise and explain why we got such weird vocabulary.\n",
    "\n",
    "**Answer**\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbt1bQ2SsK4l"
   },
   "source": [
    "### 3. Cleaning\n",
    "\n",
    "🚧 **TODO** 🚧\n",
    "\n",
    "You are going to implement some of the classical cleaning methods.\n",
    "1. Lower case the string.\n",
    "2. String normalization, i.e., replace accentuated characters by standard version (\"â -> a\") (hint: https://stackoverflow.com/a/14121678).\n",
    "3. Remove non alpha-numeric characters (use Python regex package `re`).\n",
    "4. Replace numeric characters by the token `'<NUM>'` (use the package `re`).\n",
    "5. Remove double whitespaces (use the package `re`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orSU21XPfBhL"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5EqoG1dsWMv"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    # TODO lower case\n",
    "\n",
    "    # TODO string normalization.\n",
    "\n",
    "    # TODO remove non alpha numeric characters.\n",
    "\n",
    "    # TODO replace numbers by the <NUM> token.\n",
    "\n",
    "    # TODO remove double whitespaces.\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiAbpQrbptvH"
   },
   "source": [
    "🔴 **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfmJ1GDzy7-f"
   },
   "outputs": [],
   "source": [
    "noisy_text = \"Ï   lîved  in    San-Françisco...  ! for 12 years.\"\n",
    "clean_text = \"i lived in san francisco for <NUM> years\"\n",
    "assert preprocess_text(noisy_text) == clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkO-jZPY0mDy"
   },
   "source": [
    "🚧 **TODO** 🚧\n",
    "\n",
    "Add column `clean_review` to the dataframe, with the clean text.\n",
    "\n",
    "Store the `clean_review` column to the variable `clean_column`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMO7SaQO0uSr"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "df_imdb[\"clean_review\"] = #TODO\n",
    "\n",
    "clean_column = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0Dl9i7X1JWb"
   },
   "source": [
    "🚧 **TODO** 🚧\n",
    "\n",
    "Run the tokenizer on the clean text and print 50 random tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koSGantW1SL1"
   },
   "outputs": [],
   "source": [
    "# TODO fit the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJlHz9Sw1c56"
   },
   "outputs": [],
   "source": [
    "# TODO print tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sq8CzSBw1vsI"
   },
   "source": [
    "🚧 **Question** 🚧\n",
    "\n",
    "What do you think of the new tokens? Are they better? What common noise still remain? (Feel free to print more tokens to get a better intuition).\n",
    "\n",
    "Suggest some methods to improve again the tokenization (at least 2), based on either these results or on something you think we could have done better.\n",
    "\n",
    "**Answer**\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohqx368M2ZiG"
   },
   "source": [
    "### Tokens analysis\n",
    "\n",
    "\n",
    "🚧 **TODO** 🚧\n",
    "\n",
    "Show the frequencies repartition.\n",
    "This should be a histogram, with 100 bins, with the frequencies on the x axis and the number of words associated to the frequencies on the y axis (see https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html).\n",
    "\n",
    "Use logarithm scale for y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8N76FrE3fyt"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qX-WtALB8mam"
   },
   "source": [
    "🚧 **Question** 🚧\n",
    "\n",
    "What do you think of the repartition?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXXq8T04fRYM"
   },
   "source": [
    "🚧 **TODO** 🚧\n",
    "\n",
    "Plot the frequencies repartition. Find the best corresponding Zipf law (seen during the course) and plot it on the same figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJeJUqQe_jfo"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FB--kpAVE7p_"
   },
   "source": [
    "## III - DataLoading\n",
    "\n",
    "This final part aims at making your dataset compatible with PyTorch.\n",
    "\n",
    "### Text representation\n",
    "\n",
    "Since we plan to work with PyTorch, we need to work with `tensors`. But since a text is only composed of strings, we need to find some numerical representations for the text.\n",
    "\n",
    "The most basic method is to represent a text by list of tokens. We already used a simple WhiteSpace tokenizer earlier to analyze our texts before cleaning. Now we will use it to *represent* the texts.\n",
    "\n",
    "🚧 **TODO** 🚧\n",
    "\n",
    "Modify the code of the `WhiteSpaceTokenizer` so that it could encode a text into a list of token ids and do the reverse (i.e. decode a list of token ids to a text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8a20vOKJ5Wz"
   },
   "outputs": [],
   "source": [
    "class WhiteSpaceTokenizer:\n",
    "    def __init__(self):\n",
    "        # The vocabulary will store the mapping between text tokens and their id.\n",
    "        self.vocab = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "        # We will keep track of the number of times a word appears in the corpus.\n",
    "        self.frequencies = {}\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Converts the text to a list of tokens (substrings).\"\"\"\n",
    "        # TODO\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Take a text as input and return its associated tokenization, as a list of ids.\"\"\"\n",
    "        # TODO\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        # TODO\n",
    "\n",
    "    def fit(self, corpus: List[str]):\n",
    "        \"\"\"Fits the tokenizer to a list of texts to construct its vocabulary.\"\"\"\n",
    "        current_id = 0\n",
    "        for text in tqdm(corpus):\n",
    "\n",
    "            # Split into substrings.\n",
    "            list_tokens = #TODO\n",
    "\n",
    "            for token in list_tokens:\n",
    "\n",
    "                # Add the token to the vocabulary\n",
    "                token_id = self.vocab.get(token, None)\n",
    "\n",
    "                token_not_in_vocab = #TODO\n",
    "                if token_not_in_vocab:\n",
    "                    self.vocab[token] = #TODO\n",
    "                    self.id_to_token[current_id] = #TODO\n",
    "                    self.frequencies[current_id] = 0\n",
    "                    token_id = current_id\n",
    "                    current_id += 1\n",
    "\n",
    "                self.frequencies[token_id] += 1\n",
    "\n",
    "        # Total number of words in vocab.\n",
    "        self.num_words = #TODO\n",
    "        print(f\"Built a vocabulary of {self.num_words} words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EawK5ctWqjOQ"
   },
   "source": [
    "🔴 **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1slN9MEKWjC"
   },
   "outputs": [],
   "source": [
    "tokenizer = WhiteSpaceTokenizer()\n",
    "toy_dataset = [\"the cat is in the kitchen\", \"i have a dog\"]\n",
    "tokenizer.fit(toy_dataset)\n",
    "text = \"i have a kitchen\"\n",
    "assert tokenizer.encode(text) == [5, 6, 7, 4]\n",
    "assert tokenizer.decode([0, 1, 2, 7, 8]) == \"the cat is a dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imYmiv-PMiH4"
   },
   "source": [
    "🚧 **TODO** 🚧 Now fit it on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-agmjJdMkHg"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHO2xKpiL1oI"
   },
   "source": [
    "🚧 **Question** 🚧\n",
    "Look at the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3zS9noAL0DO"
   },
   "outputs": [],
   "source": [
    "tokenizer.encode(\"the king is in the câstle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Reu2H7UXM9J_"
   },
   "source": [
    "a) Does it raise an exception? Explain what is the problem with the above situation.\n",
    "\n",
    "b) In which real life scenario could such a situation appear?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbJtCxPKOFYe"
   },
   "source": [
    "🚧 **TODO** 🚧\n",
    "\n",
    "Propose a simple modification of the tokenizer code such that it doesn't raise an exception anymore. It should handle the presence of any unknown token.\n",
    "\n",
    "The following code should't raise any exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sxrH8mFqs6I"
   },
   "source": [
    "🔴 **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJVfIsLqLQ4M"
   },
   "outputs": [],
   "source": [
    "allowed_characters = (string.ascii_uppercase + string.digits).replace(\" \", \"\")\n",
    "for _ in range(1000):\n",
    "    word_that_do_not_exist = \"\".join(random.choices(allowed_characters, k=100))\n",
    "    encoding = tokenizer.encode(f\"the king is in the {word_that_do_not_exist}\")\n",
    "    assert tokenizer.decode(encoding) == \"the king is in the <UNK>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZ_O5jT_f0Mr"
   },
   "source": [
    "### Dataset for PyTorch\n",
    "We will be working wtth PyTorch most of the time.\n",
    "A good practice is to always iterate through a `torch.utils.data.DataLoader`, coupled with `torch.utils.data.Dataset`.\n",
    "\n",
    "🚧 **TODO** 🚧\n",
    "\n",
    "We will construct our custom `Dataset` class, that should be fully compatible with the PyTorch API. According to the [documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files), such a dataset should implement at least three methods, `__init__`, `__len__`, `__getitem__`. Complete the following code such that the dataset yields a list of texts.\n",
    "\n",
    "Requirements:\n",
    "1. The dataset class should read the data from the csv_file stored in `DATA_PATH`.\n",
    "2. The dataset should output the raw text, without any cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gqn2FgOthdcF"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, file_path: str):\n",
    "        # TODO\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2kJbUUxtPi8"
   },
   "source": [
    "🔴 **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68tkYxHKiFzA"
   },
   "outputs": [],
   "source": [
    "imdb_dataset = IMDBDataset(file_path=DATA_PATH)\n",
    "text, label = imdb_dataset[0]\n",
    "assert text == text_column[0]\n",
    "assert label == label_column[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EaTm6k7QZHL"
   },
   "source": [
    "### DataLoader for PyTorch\n",
    "\n",
    "Now that the dataset is ready, we should be able to iterate through it. This is done with a `DataLoader`.\n",
    "\n",
    "The `DataLoader` also has some handy functionnalities:\n",
    "1. Batch iteration.\n",
    "2. Collate function.\n",
    "\n",
    "The collate function corresponds to the argument `collate_fn` of the constructor of `torch.utils.data.DataLoader`.\n",
    "\n",
    "This function is applied on the fly to each sample of the dataset.\n",
    "\n",
    "Our `IMDBDataset` only outputs raw texts. During the iteration through the `DataLoader`, we would like to:\n",
    "1. Clean the dataset with our cleaning function `preprocess_text`.\n",
    "2. Convert the text to a list of tokens.\n",
    "3. The `DataLoader`, for a batch size of `N`, should yield a dictionnary `{\"token_ids\": ..., \"labels\": ...,}` with `\"token_ids\"` corresponding to a list of `N` lists of tokens ids, and `\"labels\"` to a list of size `N` with the corresponding labels.\n",
    "\n",
    "🚧 **TODO** 🚧\n",
    "\n",
    "Write the `collate_fn` function so that the above requirements are satisfied (i.e., clean and tokenize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvbB5VMrSSzY"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "def collate_fn(batch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGkthDygS5uT"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0aAd6B2q4KC"
   },
   "source": [
    "🔴 **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMkufgR1Sf8J"
   },
   "outputs": [],
   "source": [
    "for batch_size in range(1, 10):\n",
    "    dataloader = DataLoader(\n",
    "        dataset=imdb_dataset, batch_size=batch_size, collate_fn=collate_fn\n",
    "    )\n",
    "    for batch in dataloader:\n",
    "        token_ids = batch[\"token_ids\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        assert len(token_ids) == batch_size\n",
    "        assert len(labels) == batch_size\n",
    "        assert isinstance(token_ids[0][0], int)\n",
    "        assert (isinstance(labels[0], int)) or isinstance(labels[0], np.int64)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JacMmD5Kq6j8"
   },
   "source": [
    "🚧 **Question** 🚧\n",
    "\n",
    "Write a concise summary on what you learned with this lab.\n",
    "\n",
    "**Answer**\n",
    "\n",
    "TODO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
